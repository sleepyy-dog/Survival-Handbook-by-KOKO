# 9.12

## 极大似然估计

### 简析：

> 对于这个函数：
> $$
> p(x|θ)
> $$
> 输入有两个：x表示某一个具体的数据； $\theta$ 表示模型的参数:
>
> 如果 $\theta$ 是已知确定的， $x$是变量，这个函数叫做概率函数(probability function)，它描述对于不同的样本点 $x$ ，其出现概率是多少。
>
> 如果 $x$ 是已知确定的， $\theta$ 是变量，这个函数叫做似然函数(likelihood function), 它描述对于不同的模型参数，出现 $x$ 这个样本点的概率是多少。
>
> 在概率统计中，概率密度函数 $p(x; \theta)$ 扮演了重要角色。当 $\theta$ 已知时， $p(x;\theta)$ 显示概率密度怎样随 $x$ 变化；反过来，当样本 $x$ 给定后，可考虑对不同的 $\theta$ ，概率密度如何变化，它反映了概率密度函数对样本 $x$ 的解释能力，这便是似然。

### 定义 1 :

设 $p(x;\theta)$， $\theta \in \Theta$ 是$(R^n, \mathcal{B}_{R^n})$上的一族联合概率密度函数，对给定的 $x$，称
$$
L(\theta; x) = k p(x; \theta)
$$
为 $\theta$ 的似然函数，其中 $k>0$ 是不依赖于 $\theta$ 的量，常取 $k=1$ 。进一步，若存在 $(R^n, \mathcal{B}_{R^n})$ 到 $(\Theta, \mathcal{B}_{\Theta})$ 的统计量 $\hat{\theta}(x)$ 使

$$
L(\hat{\theta}(x); x) = \sup_{\theta} L(\theta; x)
$$
则称 $\hat{\theta}(x)$ 为 $\theta$ 的一个极大似然估计（MLE）。

> [!NOTE]
>
> 最大似然估计和极大似然估计是完全等同的概念。

![image-20250912082024154](https://sleepy-dog-1376908035.cos.ap-guangzhou.myqcloud.com/202509120820545.png)

> #### $P156$例$4.1.4$ 
>
> #### 设总体$X$服从均匀分布$U(0,\theta)$，$\theta=(0,+\infty)$是未知参数，设$X_1,X_2,...,X_n$是取自总体$X$的样本，求参数$\theta$的极大似然估计。

#### 解：

此时
$$
L(\theta; x) =\frac{1}{\theta^n}
$$
似然函数为
$$
L(\theta)=\prod_{i=1}^{n}f(x_i;\theta)=
\begin{cases}
\frac{1}{\theta^n},0<x_i<\theta\\
0,其他
\end{cases}
$$
即$L(\theta)=\theta^{-n}I(\max\{x_i\},\theta)$，显然在$\theta=\max{x_i}$时取到最大值。

![image-20250912082641703](https://sleepy-dog-1376908035.cos.ap-guangzhou.myqcloud.com/202509120826948.png)

由于概率密度函数大多具有指数函数的形式，采用似然函数的对数通常更为简便。称
$$
l(\theta; x) = \ln L(\theta; x)
$$
为 $\theta$ 的对数似然函数。

这是因为：
$$
\log{\prod f_i}=\sum \log f_i
$$


![image-20250912083703520](https://sleepy-dog-1376908035.cos.ap-guangzhou.myqcloud.com/202509120837789.png)

> #### $P156$例$4.1.1$ 
>
> #### 设总体$X$服从指数分布$E(1/\theta)$，$\theta>0$是未知参数，来自总体$X$的样本$X_1,X_2,...,X_n$，求参数$\theta$的极大似然估计，并判别其无偏性。

#### **解：**

首先得到单个$x_i$的概率密度：
$$
p(x_i|\theta)=
\begin{cases}
\frac{1}{\theta} e^{-\frac{1}{\theta} x_i},x_i >0\\
0,其他
\end{cases}
$$
得到概率密度函数：
$$
L(\theta)=\prod_{i=1}^{n}p(x_i|\theta)=
\begin{cases}
\frac{1}{\theta^n} e^{-\theta \sum x_i},0<x_i\\
0,其他
\end{cases}
$$
解：参数的对数似然函数为 

$$
\ln L(\theta)=-n\ln\theta-\left(\sum_{i=1}^{n}x_i\right)/\theta
$$

求导得对数似然函数方程 

$$
\frac{d\ln L(\theta)}{d\theta}=-n/\theta+\left(\sum_{i=1}^{n}x_i\right)/\theta^2=0
$$

解上面的方程得 $\theta=\frac{1}{n}\sum_{i=1}^{n}x_i=\bar{x}$ 唯一的驻点，又由

$$
\frac{d^2\ln L}{d\theta^2}=\frac{n}{\theta^2}-\frac{2}{\theta^3}\left(\sum_{i=1}^{n}x_i\right)=\frac{n}{\theta^2}\left(1-\frac{2\bar{x}}{\theta}\right)<0
$$

故$\theta$的极大似然估计值是$\hat{\theta}=\bar{x}$。

![image-20250912085929249](https://sleepy-dog-1376908035.cos.ap-guangzhou.myqcloud.com/202509120859645.png)

> [!NOTE]
>
> 当 $MLE$ 存在时，寻找 MLE 最常用的方法是求导数。如果 $\hat{\theta}(x)$ 是 $\Theta$ 的**内点**，则 $\hat{\theta}(x)$ 是下列似然方程
>
> $\partial l(\theta; x) / \partial \theta_i = 0$ ，    $i = 1, \cdots, k$ 
>
> 的解。
>

![image-20250912090455544](https://sleepy-dog-1376908035.cos.ap-guangzhou.myqcloud.com/202509120904945.png)

> #### $P156$例$4.1.3$ 
>
> #### 设$X\sim N(\mu,\sigma^2)$，$\mu$，$\sigma^2$为未知参数，$x_1,\cdots,x_n$是来自$X$的一个样本值，求：$\mu$，$\sigma^2$的极大似然估计量，并判别其无偏性。

**解： **

设 $X_1, \cdots, X_n$ 是来自 $N(\mu, \sigma^2)$ 的一个样本，则

$l(\mu, \sigma^2; x) = -n\ln \sigma - \sum_{i=1}^{n}(x_i - \mu)^2 / {2\sigma^2}$ 

$\frac{\partial l}{\partial \mu} = \frac{1}{\sigma^2}(\sum_{i=1}^{n}x_i - n\mu) = 0$ 

$\frac{\partial l}{\partial \sigma^2} = \frac{\sum_{i=1}^{n} (x_i - \mu)^2}{2\sigma^4} - \frac{n}{2\sigma^2} = 0$ 

由此给出 $\mu$ 和 $\sigma^2$ 的极大似然估计为

$\hat{\mu} = \overline{x}$ ， $\hat{\sigma}^2 = \frac{1}{n}\sum_{i=1}^{n}(x - \overline{x})^2 $ 。

此时注意到$\hat{\mu}$是无偏估计，$\hat{\sigma}^2$是有偏估计。


---

#### **极大似然估计的不变性**

如果 $\hat{\theta}$ 是 $\theta$ 的极大似然估计， $g(\cdot)$ 为可测函数，则 $g(\hat{\theta})$ 也是 $g(\theta)$ 的极大似然估计。

> [!NOTE]
>
> 对于$g(\theta)$有时我们可以不考虑其完整刻画一个模型的所有参数，此时模型还是有意义且命题正确的。

## pmf（质量函数）和pdf（密度函数）的直方图估计

### 对于有限离散型：

直接计算样本点$\alpha_j$观测值的相对频率$\hat{p(\alpha_j)}$=$\alpha_j$发生的频数/总的次数

此时$\{\hat{p(\alpha_1)} ...,\hat{p(\alpha_n)}\}$即为对于pmf的估计。

### 对于无限离散型：

取定某个样本空间的值，将样本点$\alpha_j$的估计值$\hat{p(\alpha_j)}$=大于等于$\alpha_j$发生的频数/总的次数。

![image-20250912122402805](https://sleepy-dog-1376908035.cos.ap-guangzhou.myqcloud.com/202509121224581.png)

设$X_1,\cdots,X_n$是随机变量$X$的随机样本，$X$具有$cdf$ $F(x)$。在这一节，我们简略讨论样本的直方图，它是$X$的$pmf$ $p(x)$或$pdf$ $f(x)$的估计。这要取决于$X$为离散的或连续的。除了$X$作为离散或连续的随机变量，我们对$X$分布的形式做出假设。特别地，没有做出如同上面对极大似然估计值讨论那假定的参数形式。因而，所谓述的直方图被称为非参数估计量。对于非参数推断的一般讨论，参看第$10$章。首先，讨论离散情形。

当$X$分布为离散情形

假定$X$是离散随机变量，具有$pmf$ $p(x)$。首先，假如$X$的空间是有限的，比如说$D=\{a_1,\cdots,a_m\}$，$p(a_i)$的一个直观估计量是样本观测值的相对频率。它们等于$a_i$。对于$i=1,2,\cdots,m$，将统计量定义成

$$I_i(X_j)=\begin{cases}1,X_j=a_i \\ 0,X_j\neq a_i\end{cases}$$

于是，$p(a_i)$的直观估计量能够用下面均值表示成

$$\hat{p}(a_i)=\frac{1}{n}\sum_{j=1}^{n}I_i(X_j)$$

因而，估计值$(\hat{p}(a_1),\cdots,\hat{p}(a_m))$构成$pmf$ $p(x)$的非参数估计。注意到，$I_i(X_j)$服从伯努利分布，如同习题$4.1.6$证明的，$pmf$的估计量是无偏的。

其次，假如$X$空间是无限的，比如说$D=\{a_1,a_2,\cdots\}$。在应用中，我们选取一个值，比如说$a_m$，然后加以分组

$$\{a_1\},\{a_2\},\cdots,\{a_m\},\quad\bar{a}_{m+1}=\{a_{m+1},a_{m+2},\cdots\}$$

设$\hat{p}(\bar{a}_{m+1})$表示样本项中大于或等于$a_{m+1}$的比例。于是，估计值$(\hat{p}(a_1),\cdots,\hat{p}(a_m)$，$\hat{p}(\bar{a}_{m+1}))$组成了$p(x)$的估计。考虑到组的合并，一条经验法则表明，**选取$m$以使$a_m$分类的频数大于$a_{m+1}$，$a_{m+2}$，$\cdots$分类合并频数的$2$倍**。

直方图是$\hat{p}(a_i)$与$a_i$的柱状图$(barplot)$，需要考虑间隔和情况。第一种情况，假如$a_i$值表示某些属性$(category)$，例如人的头发颜色。在此情况下，各个$a_i$就不存在顺序信息。对于这种数据，通常直方图并不将各个条柱靠在一起，其高度$\hat{p}(a_i)$以各个$\hat{p}(a_i)$的递减次序画出。如此直方图经常被称为条形图$(bar\text{ }chart)$。